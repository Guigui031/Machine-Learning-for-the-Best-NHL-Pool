{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHL Player Performance Prediction - Complete ML Pipeline\n",
    "\n",
    "This notebook demonstrates a comprehensive machine learning approach for NHL player performance prediction using:\n",
    "\n",
    "## 🎯 **What We'll Cover:**\n",
    "1. **Baseline Models** - Simple, interpretable models\n",
    "2. **Advanced Models** - More sophisticated algorithms  \n",
    "3. **Ensemble Methods** - Combining multiple models\n",
    "4. **Feature Engineering** - Hockey-specific features\n",
    "5. **Model Evaluation** - Comprehensive performance analysis\n",
    "6. **Hockey-Specific Insights** - Position analysis, age curves, etc.\n",
    "\n",
    "## 📊 **Models Included:**\n",
    "- **Baseline**: Linear Regression, Ridge, Lasso, Decision Trees, k-NN\n",
    "- **Advanced**: Random Forest, XGBoost, LightGBM, SVR, Neural Networks\n",
    "- **Ensemble**: Voting, Stacking, Bagging, Custom Weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Import our ML modules\n",
    "from ml_models.features import FeatureEngineer, HockeyFeatures\n",
    "from ml_models.models import BaselineModels, AdvancedModels, EnsembleModels\n",
    "from ml_models.evaluation import ModelEvaluator\n",
    "\n",
    "# Import data pipeline\n",
    "from data_pipeline import NHLDataPipeline\n",
    "from config import config\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"🏒 Ready to build NHL prediction models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have processed data available\n",
    "processed_dir = Path(config.data.base_data_dir) / \"processed\"\n",
    "training_file = processed_dir / \"training_dataset.parquet\"\n",
    "\n",
    "if training_file.exists():\n",
    "    print(\"📊 Loading existing processed data...\")\n",
    "    df = pd.read_parquet(training_file)\n",
    "    print(f\"✅ Loaded {len(df)} training examples\")\n",
    "else:\n",
    "    print(\"📥 No processed data found. Running data pipeline...\")\n",
    "    print(\"This will take several minutes to download and process NHL data.\")\n",
    "    \n",
    "    # Initialize and run data pipeline\n",
    "    pipeline = NHLDataPipeline()\n",
    "    \n",
    "    # Download data\n",
    "    download_results = pipeline.download_all_data(force_refresh=False)\n",
    "    \n",
    "    # Get players and create training dataset\n",
    "    all_players = pipeline.get_all_players_for_seasons()\n",
    "    df = pipeline.create_training_dataset(all_players)\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"No training data could be created. Please check the data pipeline.\")\n",
    "    \n",
    "    print(f\"✅ Created {len(df)} training examples\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"\\n📋 Dataset Info:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Show sample of data\n",
    "print(f\"\\n🔍 Sample Data:\")\n",
    "display(df[['name', 'role', 'age', 'ppg_1', 'ppg_2', 'target_points']].head())\n",
    "\n",
    "# Target variable statistics\n",
    "print(f\"\\n🎯 Target Variable (Points per Game) Statistics:\")\n",
    "print(df['target_points'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create hockey-specific features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Starting feature engineering...\")\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer(scaler_type='standard')\n",
    "\n",
    "# Separate features and target\n",
    "# Remove non-predictive columns\n",
    "exclude_cols = ['player_id', 'name', 'target_points']\n",
    "X_raw = df.drop(columns=exclude_cols)\n",
    "y = df['target_points']\n",
    "\n",
    "print(f\"📊 Initial features: {len(X_raw.columns)}\")\n",
    "\n",
    "# Apply hockey-specific feature engineering\n",
    "print(\"🏒 Creating hockey-specific features...\")\n",
    "X_hockey = HockeyFeatures.create_all_hockey_features(X_raw)\n",
    "\n",
    "print(f\"📈 Features after hockey engineering: {len(X_hockey.columns)}\")\n",
    "\n",
    "# Apply general feature engineering\n",
    "print(\"⚙️ Applying feature engineering and scaling...\")\n",
    "X_engineered = feature_engineer.fit_transform(X_hockey, y)\n",
    "\n",
    "print(f\"✅ Final engineered features: {len(X_engineered.columns)}\")\n",
    "\n",
    "# Show feature importance data\n",
    "feature_info = feature_engineer.get_feature_importance_data()\n",
    "print(f\"\\n📋 Feature Engineering Summary:\")\n",
    "print(f\"  Total features: {feature_info['num_features']}\")\n",
    "print(f\"  Scaler type: {feature_info['scaler_type']}\")\n",
    "print(f\"  Pipeline fitted: {feature_info['is_fitted']}\")\n",
    "\n",
    "# Display feature groups\n",
    "feature_groups = HockeyFeatures.get_feature_groups()\n",
    "print(f\"\\n📊 Feature Groups Available:\")\n",
    "for group_name, patterns in feature_groups.items():\n",
    "    matching_features = [col for col in X_engineered.columns \n",
    "                        for pattern in patterns if pattern in col]\n",
    "    if matching_features:\n",
    "        print(f\"  {group_name}: {len(matching_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Split data for proper evaluation (temporal split to avoid data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"📊 Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"\\n🎯 Target Distribution:\")\n",
    "print(f\"  Training mean: {y_train.mean():.3f} ± {y_train.std():.3f}\")\n",
    "print(f\"  Test mean: {y_test.mean():.3f} ± {y_test.std():.3f}\")\n",
    "\n",
    "# Convert to numpy arrays for sklearn compatibility\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "y_train_np = y_train.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "print(f\"✅ Data prepared for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "\n",
    "Start with simple, interpretable baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 Training Baseline Models...\")\n",
    "\n",
    "# Initialize baseline models\n",
    "baseline = BaselineModels(random_state=42)\n",
    "\n",
    "# Get all baseline models\n",
    "baseline_models = baseline.get_all_baseline_models()\n",
    "print(f\"📊 Training {len(baseline_models)} baseline models...\")\n",
    "\n",
    "# Train models (with hyperparameter tuning)\n",
    "fitted_baseline = baseline.fit_all_models(X_train_np, y_train_np, tune=True, cv=5)\n",
    "\n",
    "print(f\"✅ Successfully trained {len(fitted_baseline)} baseline models\")\n",
    "\n",
    "# Evaluate baseline models\n",
    "print(\"\\n📈 Evaluating baseline models...\")\n",
    "baseline_scores = baseline.evaluate_models(X_test_np, y_test_np)\n",
    "\n",
    "# Display results\n",
    "baseline_summary = baseline.get_model_summary()\n",
    "print(\"\\n🏆 Baseline Model Results:\")\n",
    "display(baseline_summary.round(4))\n",
    "\n",
    "# Get best baseline model\n",
    "best_baseline_name, best_baseline_model, best_baseline_score = baseline.get_best_model('rmse')\n",
    "print(f\"\\n🥇 Best baseline model: {best_baseline_name} (RMSE: {best_baseline_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Models\n",
    "\n",
    "Train more sophisticated models for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Training Advanced Models...\")\n",
    "\n",
    "# Initialize advanced models\n",
    "advanced = AdvancedModels(random_state=42)\n",
    "\n",
    "# Get all advanced models\n",
    "advanced_models = advanced.get_all_advanced_models()\n",
    "print(f\"📊 Training {len(advanced_models)} advanced models...\")\n",
    "\n",
    "# Train models (with hyperparameter tuning for key models)\n",
    "fitted_advanced = advanced.fit_all_models(X_train_np, y_train_np, tune=True, cv=5)\n",
    "\n",
    "print(f\"✅ Successfully trained {len(fitted_advanced)} advanced models\")\n",
    "\n",
    "# Evaluate advanced models\n",
    "print(\"\\n📈 Evaluating advanced models...\")\n",
    "advanced_scores = advanced.evaluate_models(X_test_np, y_test_np)\n",
    "\n",
    "# Display results\n",
    "advanced_summary = advanced.get_model_summary()\n",
    "print(\"\\n🏆 Advanced Model Results:\")\n",
    "display(advanced_summary.round(4))\n",
    "\n",
    "# Get best advanced model\n",
    "best_advanced_name, best_advanced_model, best_advanced_score = advanced.get_best_model('rmse')\n",
    "print(f\"\\n🥇 Best advanced model: {best_advanced_name} (RMSE: {best_advanced_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Models\n",
    "\n",
    "Combine multiple models for potentially better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎭 Training Ensemble Models...\")\n",
    "\n",
    "# Initialize ensemble models\n",
    "ensemble = EnsembleModels(random_state=42)\n",
    "\n",
    "# Get all ensemble models\n",
    "ensemble_models = ensemble.get_all_ensemble_models()\n",
    "print(f\"📊 Training {len(ensemble_models)} ensemble models...\")\n",
    "\n",
    "# Train ensemble models\n",
    "fitted_ensembles = ensemble.fit_ensemble_models(X_train_np, y_train_np)\n",
    "\n",
    "print(f\"✅ Successfully trained {len(fitted_ensembles)} ensemble models\")\n",
    "\n",
    "# Create adaptive ensemble\n",
    "print(\"\\n🧠 Creating adaptive ensemble...\")\n",
    "adaptive_ensemble = ensemble.create_adaptive_ensemble(X_train_np, y_train_np, validation_size=0.2)\n",
    "if adaptive_ensemble:\n",
    "    fitted_ensembles['adaptive'] = adaptive_ensemble\n",
    "    print(\"✅ Adaptive ensemble created\")\n",
    "\n",
    "# Evaluate ensemble models\n",
    "print(\"\\n📈 Evaluating ensemble models...\")\n",
    "ensemble_scores = ensemble.evaluate_ensembles(X_test_np, y_test_np)\n",
    "\n",
    "# Display results\n",
    "ensemble_summary = ensemble.get_ensemble_summary()\n",
    "print(\"\\n🏆 Ensemble Model Results:\")\n",
    "display(ensemble_summary.round(4))\n",
    "\n",
    "# Get best ensemble model\n",
    "best_ensemble_name, best_ensemble_model, best_ensemble_score = ensemble.get_best_ensemble('rmse')\n",
    "print(f\"\\n🥇 Best ensemble model: {best_ensemble_name} (RMSE: {best_ensemble_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation\n",
    "\n",
    "Compare all models and generate detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Comprehensive Model Evaluation...\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Combine all fitted models\n",
    "all_models = {}\n",
    "all_models.update({f\"baseline_{k}\": v for k, v in fitted_baseline.items()})\n",
    "all_models.update({f\"advanced_{k}\": v for k, v in fitted_advanced.items()})\n",
    "all_models.update({f\"ensemble_{k}\": v for k, v in fitted_ensembles.items()})\n",
    "\n",
    "print(f\"🔍 Evaluating {len(all_models)} total models...\")\n",
    "\n",
    "# Generate comprehensive comparison\n",
    "comparison_results = evaluator.compare_models(all_models, X_test_np, y_test_np)\n",
    "\n",
    "print(\"\\n🏆 Final Model Comparison (Top 10):\")\n",
    "display(comparison_results.head(10).round(4))\n",
    "\n",
    "# Find overall best model\n",
    "overall_best = comparison_results.iloc[0]\n",
    "print(f\"\\n🥇 Overall Best Model: {overall_best['model']}\")\n",
    "print(f\"   RMSE: {overall_best['rmse']:.4f}\")\n",
    "print(f\"   R²: {overall_best['r2']:.4f}\")\n",
    "print(f\"   MAE: {overall_best['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "print(\"📈 Creating model comparison visualizations...\")\n",
    "evaluator.plot_model_comparison(figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs actual for top 6 models\n",
    "top_models = comparison_results.head(6)['model'].tolist()\n",
    "print(f\"📊 Plotting predictions vs actual for top {len(top_models)} models...\")\n",
    "evaluator.plot_predictions_vs_actual(model_names=top_models, figsize=(18, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for top models\n",
    "print(\"🔍 Residual analysis for top models...\")\n",
    "evaluator.plot_residuals(model_names=top_models[:4], figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for interpretable models\n",
    "interpretable_models = {\n",
    "    'Random Forest': fitted_advanced.get('random_forest'),\n",
    "    'Decision Tree': fitted_baseline.get('decision_tree')\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if 'xgboost' in fitted_advanced:\n",
    "    interpretable_models['XGBoost'] = fitted_advanced['xgboost']\n",
    "\n",
    "interpretable_models = {k: v for k, v in interpretable_models.items() if v is not None}\n",
    "\n",
    "if interpretable_models:\n",
    "    print(f\"🔧 Feature importance analysis for {len(interpretable_models)} models...\")\n",
    "    evaluator.plot_feature_importance(\n",
    "        interpretable_models, feature_names, top_k=15, figsize=(16, 10)\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ No interpretable models available for feature importance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hockey-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by player position\n",
    "print(\"🏒 Hockey-Specific Analysis...\")\n",
    "\n",
    "# Get the best model for analysis\n",
    "best_model = all_models[overall_best['model']]\n",
    "best_predictions = best_model.predict(X_test_np)\n",
    "\n",
    "# Create analysis dataframe\n",
    "analysis_df = pd.DataFrame({\n",
    "    'actual': y_test_np,\n",
    "    'predicted': best_predictions,\n",
    "    'role': X_test['role'].values if 'role' in X_test.columns else 'Unknown',\n",
    "    'age': X_test['age'].values if 'age' in X_test.columns else np.nan\n",
    "})\n",
    "\n",
    "analysis_df['residual'] = analysis_df['actual'] - analysis_df['predicted']\n",
    "analysis_df['absolute_error'] = np.abs(analysis_df['residual'])\n",
    "\n",
    "# Performance by position\n",
    "if 'role' in analysis_df.columns and analysis_df['role'].nunique() > 1:\n",
    "    print(\"\\n📊 Model Performance by Position:\")\n",
    "    position_performance = analysis_df.groupby('role').agg({\n",
    "        'absolute_error': ['mean', 'std'],\n",
    "        'residual': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    display(position_performance)\n",
    "    \n",
    "    # Visualize performance by position\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Box plot of errors by position\n",
    "    analysis_df.boxplot(column='absolute_error', by='role', ax=axes[0])\n",
    "    axes[0].set_title('Prediction Error by Position')\n",
    "    axes[0].set_xlabel('Position')\n",
    "    axes[0].set_ylabel('Absolute Error')\n",
    "    \n",
    "    # Scatter plot of actual vs predicted by position\n",
    "    for role in analysis_df['role'].unique():\n",
    "        role_data = analysis_df[analysis_df['role'] == role]\n",
    "        axes[1].scatter(role_data['actual'], role_data['predicted'], \n",
    "                       alpha=0.6, label=f'Position {role}')\n",
    "    \n",
    "    axes[1].plot([analysis_df['actual'].min(), analysis_df['actual'].max()], \n",
    "                [analysis_df['actual'].min(), analysis_df['actual'].max()], \n",
    "                'r--', label='Perfect Prediction')\n",
    "    axes[1].set_xlabel('Actual Points per Game')\n",
    "    axes[1].set_ylabel('Predicted Points per Game')\n",
    "    axes[1].set_title('Predictions by Position')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Performance by age groups\n",
    "if not analysis_df['age'].isna().all():\n",
    "    analysis_df['age_group'] = pd.cut(analysis_df['age'], \n",
    "                                     bins=[0, 23, 27, 31, 50], \n",
    "                                     labels=['Young (≤23)', 'Prime (24-27)', 'Veteran (28-31)', 'Old (32+)'])\n",
    "    \n",
    "    print(\"\\n📊 Model Performance by Age Group:\")\n",
    "    age_performance = analysis_df.groupby('age_group').agg({\n",
    "        'absolute_error': ['mean', 'std', 'count'],\n",
    "        'residual': ['mean']\n",
    "    }).round(4)\n",
    "    display(age_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "print(\"📋 Generating Comprehensive Evaluation Report...\")\n",
    "\n",
    "evaluation_report = evaluator.generate_evaluation_report(\n",
    "    all_models, X_test_np, y_test_np, feature_names\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "summary = evaluation_report['summary']\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n🏆 Best Models:\")\n",
    "best_models = evaluation_report['best_models']\n",
    "for metric, model in best_models.items():\n",
    "    print(f\"  {metric}: {model}\")\n",
    "\n",
    "print(\"\\n💡 Recommendations:\")\n",
    "for i, recommendation in enumerate(evaluation_report['recommendations'], 1):\n",
    "    print(f\"  {i}. {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model for Production Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model and feature engineer\n",
    "models_dir = Path(\"models_saved\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "best_model_name = overall_best['model']\n",
    "best_model_obj = all_models[best_model_name]\n",
    "\n",
    "model_path = models_dir / f\"best_nhl_model_{best_model_name.replace('_', '-')}.joblib\"\n",
    "joblib.dump(best_model_obj, model_path)\n",
    "\n",
    "# Save feature engineer\n",
    "feature_engineer_path = models_dir / \"feature_engineer.joblib\"\n",
    "joblib.dump(feature_engineer, feature_engineer_path)\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'performance_metrics': {\n",
    "        'rmse': float(overall_best['rmse']),\n",
    "        'r2': float(overall_best['r2']),\n",
    "        'mae': float(overall_best['mae'])\n",
    "    },\n",
    "    'feature_names': feature_names,\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'target_mean': float(y.mean()),\n",
    "    'target_std': float(y.std())\n",
    "}\n",
    "\n",
    "metadata_path = models_dir / \"model_metadata.json\"\n",
    "import json\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"💾 Model artifacts saved:\")\n",
    "print(f\"  Best model: {model_path}\")\n",
    "print(f\"  Feature engineer: {feature_engineer_path}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n🎯 Production Ready!\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Performance: RMSE={overall_best['rmse']:.4f}, R²={overall_best['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Making Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the saved model for predictions\n",
    "print(\"🔮 Example: Making Predictions on New Data\")\n",
    "\n",
    "# Load saved model and feature engineer\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_feature_engineer = joblib.load(feature_engineer_path)\n",
    "\n",
    "# Use current season data if available\n",
    "current_file = processed_dir / f\"current_season_{config.data.current_season}.parquet\"\n",
    "\n",
    "if current_file.exists():\n",
    "    print(f\"📊 Loading current season data for predictions...\")\n",
    "    current_df = pd.read_parquet(current_file)\n",
    "    \n",
    "    # Prepare features (same preprocessing as training)\n",
    "    exclude_cols = ['player_id', 'name'] + (['target_points'] if 'target_points' in current_df.columns else [])\n",
    "    X_current_raw = current_df.drop(columns=[col for col in exclude_cols if col in current_df.columns])\n",
    "    \n",
    "    # Apply same feature engineering\n",
    "    X_current_hockey = HockeyFeatures.create_all_hockey_features(X_current_raw)\n",
    "    X_current_processed = loaded_feature_engineer.transform(X_current_hockey)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = loaded_model.predict(X_current_processed.values)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'player_name': current_df['name'].values,\n",
    "        'position': current_df['role'].values if 'role' in current_df.columns else 'Unknown',\n",
    "        'predicted_ppg': predictions\n",
    "    })\n",
    "    \n",
    "    # Sort by predicted performance\n",
    "    results_df = results_df.sort_values('predicted_ppg', ascending=False)\n",
    "    \n",
    "    print(f\"\\n🏆 Top 10 Predicted Performers for {config.data.current_season}:\")\n",
    "    display(results_df.head(10))\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions_path = models_dir / f\"predictions_{config.data.current_season}.csv\"\n",
    "    results_df.to_csv(predictions_path, index=False)\n",
    "    print(f\"\\n💾 Predictions saved to: {predictions_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"📝 No current season data available for predictions.\")\n",
    "    print(\"Run the data pipeline first to download current season data.\")\n",
    "\n",
    "print(\"\\n✅ Model training and evaluation complete!\")\n",
    "print(\"🚀 Ready for NHL pool optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### 🎯 **What We Accomplished:**\n",
    "\n",
    "1. **✅ Data Pipeline**: Loaded and processed NHL player data\n",
    "2. **✅ Feature Engineering**: Created hockey-specific features\n",
    "3. **✅ Baseline Models**: Trained simple, interpretable models\n",
    "4. **✅ Advanced Models**: Applied sophisticated ML algorithms\n",
    "5. **✅ Ensemble Methods**: Combined models for better performance\n",
    "6. **✅ Evaluation**: Comprehensive model comparison and analysis\n",
    "7. **✅ Production Ready**: Saved best model for deployment\n",
    "\n",
    "### 📊 **Model Performance:**\n",
    "- **Best Model**: See results above\n",
    "- **Key Insights**: Position-specific performance patterns\n",
    "- **Ready for**: Pool optimization and player selection\n",
    "\n",
    "### 🚀 **Next Steps:**\n",
    "1. Use predictions for team optimization\n",
    "2. Integrate with salary constraints \n",
    "3. Build automated prediction pipeline\n",
    "4. Monitor model performance over time\n",
    "\n",
    "**Your NHL prediction models are ready to help build the optimal fantasy team! 🏒**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}