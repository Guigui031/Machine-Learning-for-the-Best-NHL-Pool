# NHL Pool Optimization System

A machine learning-powered system for optimizing NHL fantasy hockey team selection using predictive analytics and mathematical optimization.

## ğŸ’ Overview

This system predicts NHL player performance and selects optimal fantasy teams within salary and position constraints using:
- **Machine Learning**: Ensemble models (XGBoost, SVR, SGD) for PPG prediction
- **Optimization**: Linear Programming and Branch & Bound algorithms
- **Data Pipeline**: Automated NHL API data collection and processing

## ğŸš€ Quick Start

### Installation
```bash
pip install -r requirements.txt
```

### Main Workflows

#### 1. Complete ML Pipeline
```bash
jupyter notebook NHL_ML_Models_Complete.ipynb
```
- Data collection and preprocessing
- Feature engineering and model training
- Performance evaluation and model selection

#### 2. Team Optimization
```bash
jupyter notebook Team_Optimization_Notebook.ipynb
```
- Load trained models and generate predictions
- Apply salary cap and position constraints
- Generate optimal team composition

#### 3. Original Analysis
```bash
jupyter notebook ML_for_NHL.ipynb
```
- Historical analysis and model development
- Exploratory data analysis
- Algorithm comparison

### Individual Components
```bash
# Test data download
python data_download.py

# Test data processing
python process_data.py

# Test ML models
python model_predictor.py

# Test optimization
python pool_classifier.py
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ ğŸ“Š Core Notebooks
â”‚   â”œâ”€â”€ NHL_ML_Models_Complete.ipynb    # Complete ML pipeline
â”‚   â”œâ”€â”€ Team_Optimization_Notebook.ipynb  # Team optimization
â”‚   â””â”€â”€ ML_for_NHL.ipynb                # Original analysis
â”‚
â”œâ”€â”€ ğŸ”§ Core Modules
â”‚   â”œâ”€â”€ data_download.py                # NHL API data fetching
â”‚   â”œâ”€â”€ process_data.py                 # Data processing & normalization
â”‚   â”œâ”€â”€ player.py                       # Player & Season classes
â”‚   â”œâ”€â”€ team.py                         # Team data model
â”‚   â”œâ”€â”€ ensemble_learning.py            # ML ensemble methods
â”‚   â”œâ”€â”€ model_predictor.py             # Model prediction interface
â”‚   â”œâ”€â”€ pool_classifier.py             # Team optimization
â”‚   â”œâ”€â”€ data_validation.py             # Data quality validation
â”‚   â””â”€â”€ config.py                      # Configuration settings
â”‚
â”œâ”€â”€ ğŸ¤– ML Models (Structured)
â”‚   â”œâ”€â”€ ml_models/                     # Modern ML architecture
â”‚   â”‚   â”œâ”€â”€ models/                    # Model implementations
â”‚   â”‚   â”œâ”€â”€ features/                  # Feature engineering
â”‚   â”‚   â”œâ”€â”€ evaluation/                # Model evaluation
â”‚   â”‚   â””â”€â”€ utils/                     # ML utilities
â”‚   â””â”€â”€ models_saved/                  # Trained model artifacts
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ docs/                          # Comprehensive documentation
â”‚   â”‚   â”œâ”€â”€ data-module.md
â”‚   â”‚   â”œâ”€â”€ models-module.md
â”‚   â”‚   â”œâ”€â”€ optimizer-module.md
â”‚   â”‚   â”œâ”€â”€ core-classes.md
â”‚   â”‚   â”œâ”€â”€ workflow-documentation.md
â”‚   â”‚   â””â”€â”€ graphs/                    # Visualization assets
â”‚   â””â”€â”€ CLAUDE.md                      # Development guide
â”‚
â”œâ”€â”€ ğŸ—ï¸ Infrastructure
â”‚   â”œâ”€â”€ src/                           # Legacy structured code
â”‚   â”œâ”€â”€ requirements.txt               # Dependencies
â”‚   â””â”€â”€ archives/                      # Archived files
â”‚       â”œâ”€â”€ old_notebooks/
â”‚       â”œâ”€â”€ test_files/
â”‚       â”œâ”€â”€ result_files/
â”‚       â”œâ”€â”€ legacy_code/
â”‚       â””â”€â”€ documentation/
```

## ğŸ¯ Key Features

### Machine Learning Pipeline
- **Ensemble Methods**: XGBoost, SVR, SGD, Logistic Regression
- **Feature Engineering**: Historical performance, player attributes, team context
- **Model Validation**: Cross-validation with hyperparameter tuning
- **Performance**: RÂ² scores up to 0.768 for PPG prediction

### Optimization Engine
- **Linear Programming**: Guaranteed optimal solutions using PuLP
- **Branch & Bound**: Custom implementation for educational purposes
- **Constraints**: $88M salary cap, position limits (12A/6D/2G)
- **Efficiency**: Optimal team selection in <1 second

### Data Management
- **NHL API Integration**: Official data with robust error handling
- **Multi-Season Support**: Historical data spanning 4+ seasons
- **Data Validation**: Comprehensive quality checks and cleaning
- **Caching**: Efficient data storage and retrieval

## ğŸ“Š Performance Metrics

| Component | Metric | Value |
|-----------|--------|-------|
| **ML Models** | Best RÂ² Score | 0.768 (Ensemble) |
| **Optimization** | Solve Time | <0.2 seconds (LP) |
| **Data Coverage** | Players | ~800 active NHL players |
| **Constraints** | Salary Cap | $88M with position limits |

## ğŸ”§ Configuration

Key settings in `config.py` and `CLAUDE.md`:
- NHL API endpoints and rate limits
- Model hyperparameter spaces
- Optimization constraints
- Data validation rules

## ğŸš§ Development

### Testing
```bash
# Individual component testing (archived)
# See archives/test_files/ for historical tests
```

### Documentation
- **Module Documentation**: See `docs/` folder
- **Visual Guides**: Generated charts in `docs/graphs/`
- **API Reference**: Docstrings in all core modules

## ğŸ“ˆ Results

The system typically generates:
- **Optimal Teams**: 20 players maximizing predicted PPG
- **Performance**: ~250 total PPG for optimal lineups
- **Efficiency**: 95-98% salary cap utilization
- **Accuracy**: Validated through backtesting on historical data

## ğŸ¤ Contributing

1. Follow existing code structure and documentation standards
2. Add comprehensive docstrings and type hints
3. Include tests for new functionality
4. Update relevant documentation in `docs/`

## ğŸ“ Support

- **Documentation**: See `docs/` folder for detailed guides
- **Configuration**: Check `CLAUDE.md` for development setup
- **Issues**: Review code structure and validation logic

---

**Built for optimal NHL fantasy team selection through data-driven decision making.**